{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "284603ea-0a2c-41cc-8157-30caf0bc4950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the PySpark environment variables\n",
    "import os\n",
    "os.environ['SPARK_HOME'] = \"/Users/venkatasaisabbineni/Desktop/Spark\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93db6e82-0612-4487-bd21-4425c253f4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/05 16:04:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Import PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('FromCSV').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e04de95-ebe8-480a-8bc7-232d972dd80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"./data/products.csv\"\n",
    "df = spark.read.csv(csv_file_path,header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e52e6df-3144-49a6-92fd-72233e1fd6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n",
      "+---+--------------------+---------------+--------+------+\n",
      "| id|                name|       category|quantity| price|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "|  1|           iPhone 12|    Electronics|      10|899.99|\n",
      "|  2|     Nike Air Max 90|       Clothing|      25|119.99|\n",
      "|  3|KitchenAid Stand ...|Home Appliances|       5|299.99|\n",
      "|  4|    The Great Gatsby|          Books|      50| 12.99|\n",
      "|  5|L'Oreal Paris Mas...|         Beauty|     100|  9.99|\n",
      "|  6|            Yoga Mat|         Sports|      30| 29.99|\n",
      "|  7| Samsung 4K Smart TV|    Electronics|       8|799.99|\n",
      "|  8|        Levi's Jeans|       Clothing|      15| 49.99|\n",
      "|  9|Dyson Vacuum Cleaner|Home Appliances|       3|399.99|\n",
      "| 10| Harry Potter Series|          Books|      20| 15.99|\n",
      "| 11|        MAC Lipstick|         Beauty|      75| 16.99|\n",
      "| 12|Adidas Running Shoes|         Sports|      22| 59.99|\n",
      "| 13|       PlayStation 5|    Electronics|      12|499.99|\n",
      "| 14|   Hooded Sweatshirt|       Clothing|      10| 34.99|\n",
      "| 15|        Coffee Maker|Home Appliances|       7| 89.99|\n",
      "| 16|To Kill a Mocking...|          Books|      15|  9.99|\n",
      "| 17|        Skincare Set|         Beauty|      50| 49.99|\n",
      "| 18|           Yoga Ball|         Sports|      18| 19.99|\n",
      "| 19|Sony Noise-Cancel...|    Electronics|       6|299.99|\n",
      "| 20|        Puma T-shirt|       Clothing|      40| 19.99|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97bf37c1-0a88-4912-b7f9-24efb43ea060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'This', 'This', 'is', 'This']\n",
      "[('is', 4), ('Hello', 4), ('This', -4)]\n",
      "('is', 4)\n",
      "('Hello', 4)\n",
      "('This', -4)\n"
     ]
    }
   ],
   "source": [
    "words = [\"Hello\",\"This\",\"This\",\"is\",\"This\"]\n",
    "rdd = spark.sparkContext.parallelize(words)\n",
    "a=rdd.collect()\n",
    "print(a)\n",
    "#df= spark.createDataFrame(words,[\"name\"])\n",
    "#df.show()\n",
    "wordCounts = rdd.map(lambda word:(word,4)).reduceByKey(lambda x,y:x+y)\n",
    "print(wordCounts.collect())\n",
    "for word in wordCounts.collect():\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5d06808-13b9-4290-870c-8dd8a1176943",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
